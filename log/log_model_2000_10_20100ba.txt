2022-07-24 22:06:10,773:INFO:Loading graph erdos_renyi
2022-07-24 22:06:10,774:INFO:Loading agent...
/home/zhihao/Document/RLGNN/COLGE_/utils/config.py:6: YAMLLoadWarning: calling yaml.load() without Loader=... is deprecated, as the default Loader is unsafe. Please read https://msg.pyyaml.org/load for full details.
  model_config = yaml.load(config_file)
2022-07-24 22:06:13,682:INFO:Loading environment SIR
Loading training data: graph_dic_10_20100ba
<agent.DQAgent object at 0x7fa1b888ea00>
Running a single instance simulation...
 -> epoch : 0
 -> games : 0
20
[2, 3, 8]
[ 0  1  4  5  6  7  9 10 11 12 13 14 15 16 17 18 19]
[ 0  1  4  5  6  7  9 10 11 12 13 14 15 16 17 18]
[ 0  4  5  6  7  9 10 11 12 13 14 15 16 17 18]
[ 4  5  6  7  9 10 11 12 13 14 15 16 17 18]
[ 4  5  6  7  9 10 11 12 14 15 16 17 18]
[ 4  5  6  7  9 10 11 12 15 16 17 18]
 ->    Terminal event: cumulative rewards = -0.16000000000000014
 -> games : 1
20
[17, 19, 5]
[ 0  1  2  3  4  6  7  8  9 10 11 12 13 14 15 16 18]
[ 0  1  2  3  4  6  7  8  9 10 11 12 13 14 15 16]
[ 0  1  2  3  4  6  7  9 10 11 12 13 14 15 16]
[ 0  1  2  3  4  6  7  9 10 11 12 14 15 16]
[ 0  1  2  3  4  6  7  9 10 12 14 15 16]
[ 0  2  3  4  6  7  9 10 12 14 15 16]
 ->    Terminal event: cumulative rewards = 0.019999999999997797
 -> games : 2
40
[6, 33, 3]
[ 0  1  2  4  5  7  8  9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25
 26 27 28 29 30 31 32 34 35 36 37 38 39]
[ 0  1  2  4  5  7  8  9 10 11 12 13 14 15 16 17 18 20 21 22 23 24 25 26
 27 28 29 30 31 32 34 35 36 37 38 39]
[ 0  1  2  4  5  7  8  9 10 11 12 13 14 15 16 17 20 21 22 23 24 25 26 27
 28 29 30 31 32 34 35 36 37 38 39]
[ 0  1  2  4  5  7  8  9 10 11 12 14 15 16 17 20 21 22 23 24 25 26 27 28
 29 30 31 32 34 35 36 37 38 39]
[ 0  1  2  4  5  7  8  9 10 11 12 14 15 16 17 20 21 22 23 24 26 27 28 29
 30 31 32 34 35 36 37 38 39]
[ 0  1  2  4  5  7  8  9 11 12 14 15 16 17 20 21 22 23 24 26 27 28 29 30
 31 32 34 35 36 37 38 39]
[ 0  1  2  4  5  7  8  9 11 12 14 15 17 20 21 22 23 24 26 27 28 29 30 31
 32 34 35 36 37 38 39]
[ 0  1  2  4  5  7  8  9 11 12 14 15 17 20 21 22 23 24 26 27 28 29 30 31
 34 35 36 37 38 39]
[ 0  1  2  4  5  7  8  9 11 12 14 15 17 21 22 23 24 26 27 28 29 30 31 34
 35 36 37 38 39]
[ 0  1  2  4  5  7  8  9 11 12 14 15 17 21 22 23 24 26 27 28 29 30 31 34
 35 36 37 39]
[ 0  1  2  4  5  7  8  9 11 12 14 15 17 21 22 23 24 26 27 29 30 31 34 35
 36 37 39]
[ 0  1  2  4  5  7  8  9 11 12 14 15 17 22 23 24 26 27 29 30 31 34 35 36
 37 39]
 ->    Terminal event: cumulative rewards = 1.1599999999999966
 -> games : 3
40
[16, 2, 10]
[ 0  1  3  4  5  6  7  8  9 11 12 13 14 15 17 18 19 20 21 22 23 24 25 26
 27 28 29 30 31 32 33 34 35 36 37 38 39]
[ 0  1  3  4  5  6  7  8  9 11 12 13 14 15 17 18 19 20 21 22 23 24 25 26
 27 28 29 30 31 32 33 34 35 37 38 39]
[ 0  1  3  4  5  6  7  8  9 11 12 13 14 15 17 18 19 20 21 22 23 24 25 26
 27 28 29 30 31 32 33 34 37 38 39]
[ 0  1  3  4  5  6  7  8  9 11 12 13 14 15 17 18 19 20 21 22 23 24 26 27
 28 29 30 31 32 33 34 37 38 39]
[ 0  1  3  4  5  6  7  8  9 11 12 13 14 15 17 18 19 20 21 22 23 24 26 27
 28 29 30 31 32 33 34 37 39]
[ 0  1  3  4  5  6  7  8  9 11 12 13 14 15 17 18 19 20 21 23 24 26 27 28
 29 30 31 32 33 34 37 39]
[ 0  1  3  4  5  6  7  8  9 11 12 13 14 15 17 18 19 20 21 23 24 26 28 29
 30 31 32 33 34 37 39]
[ 0  1  3  4  5  6  7  8  9 11 12 13 14 15 17 18 19 20 21 23 24 26 29 30
 31 32 33 34 37 39]
[ 0  1  3  4  5  6  7  8  9 11 12 13 14 15 18 19 20 21 23 24 26 29 30 31
 32 33 34 37 39]
[ 0  1  3  4  5  6  7  8  9 11 13 14 15 18 19 20 21 23 24 26 29 30 31 32
 33 34 37 39]
[ 0  1  3  4  5  6  7  8  9 11 13 14 15 18 19 20 21 23 26 29 30 31 32 33
 34 37 39]
[ 0  1  3  4  5  6  7  8  9 11 13 14 15 18 19 20 23 26 29 30 31 32 33 34
 37 39]
 ->    Terminal event: cumulative rewards = 0.7999999999999972
 -> games : 4
60
[25, 36, 32]
[ 0  1  2  3  4  5  6  7  8  9 10 11 12 13 14 15 16 17 18 19 20 21 22 23
 24 26 27 28 29 30 31 33 34 35 37 38 39 40 41 42 43 44 45 46 47 48 49 50
 51 52 53 54 55 56 57 58 59]
[ 0  1  2  3  4  5  6  7  8  9 10 11 12 13 14 15 16 17 18 19 21 22 23 24
 26 27 28 29 30 31 33 34 35 37 38 39 40 41 42 43 44 45 46 47 48 49 50 51
 52 53 54 55 56 57 58 59]
[ 0  1  2  3  4  5  6  7  8  9 10 11 12 13 14 15 16 17 18 19 21 22 23 24
 26 28 29 30 31 33 34 35 37 38 39 40 41 42 43 44 45 46 47 48 49 50 51 52
 53 54 55 56 57 58 59]
[ 0  1  2  3  4  5  6  7  8  9 10 11 12 13 14 15 16 17 18 19 21 22 23 24
 26 28 29 30 31 33 34 35 37 38 39 40 41 42 43 44 45 46 47 48 49 50 51 52
 53 54 55 56 57 58]
[ 0  1  2  3  4  5  6  7  8  9 10 11 12 13 15 16 17 18 19 21 22 23 24 26
 28 29 30 31 33 34 35 37 38 39 40 41 42 43 44 45 46 47 48 49 50 51 52 53
 54 55 56 57 58]
[ 0  1  2  3  4  5  6  7  8  9 10 11 12 13 15 16 17 19 21 22 23 24 26 28
 29 30 31 33 34 35 37 38 39 40 41 42 43 44 45 46 47 48 49 50 51 52 53 54
 55 56 57 58]
[ 1  2  3  4  5  6  7  8  9 10 11 12 13 15 16 17 19 21 22 23 24 26 28 29
 30 31 33 34 35 37 38 39 40 41 42 43 44 45 46 47 48 49 50 51 52 53 54 55
 56 57 58]
[ 1  2  3  4  5  6  7  8  9 10 11 12 13 15 16 17 19 21 23 24 26 28 29 30
 31 33 34 35 37 38 39 40 41 42 43 44 45 46 47 48 49 50 51 52 53 54 55 56
 57 58]
[ 1  2  3  4  5  6  7  8  9 10 11 12 13 15 16 17 19 21 23 24 26 28 29 30
 31 33 34 35 37 38 40 41 42 43 44 45 46 47 48 49 50 51 52 53 54 55 56 57
 58]
[ 2  3  4  5  6  7  8  9 10 11 12 13 15 16 17 19 21 23 24 26 28 29 30 31
 33 34 35 37 38 40 41 42 43 44 45 46 47 48 49 50 51 52 53 54 55 56 57 58]
[ 2  3  4  5  6  7  8  9 10 11 12 13 15 16 17 19 21 23 24 26 29 30 31 33
 34 35 37 38 40 41 42 43 44 45 46 47 48 49 50 51 52 53 54 55 56 57 58]
[ 2  3  4  5  6  7  8  9 10 11 12 13 15 17 19 21 23 24 26 29 30 31 33 34
 35 37 38 40 41 42 43 44 45 46 47 48 49 50 51 52 53 54 55 56 57 58]
Traceback (most recent call last):
  File "main.py", line 89, in <module>
    main()
  File "main.py", line 82, in main
    final_reward = my_runner.loop(args.ngames,args.epoch, args.niter,args.savedname)
  File "/home/zhihao/Document/RLGNN/COLGE_/runner.py", line 83, in loop
    (obs, act, rew, done) = self.step()
  File "/home/zhihao/Document/RLGNN/COLGE_/runner.py", line 23, in step
    self.agent.reward(observation, action, reward,done)
  File "/home/zhihao/Document/RLGNN/COLGE_/agent.py", line 140, in reward
    (last_observation_tens, action_tens, reward_tens, observation_tens, done_tens,adj_tens) = self.get_sample()
  File "/home/zhihao/Document/RLGNN/COLGE_/agent.py", line 188, in get_sample
    last_observation_tens=torch.cat((last_observation_tens,last_observation_))
RuntimeError: Sizes of tensors must match except in dimension 0. Expected size 40 but got size 20 for tensor number 1 in the list.
